{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with Pandas library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I'll discuss how to carry out data aquisation and obtain various data insghts using the pandas library. Here data will be imported into Jupyter notebook and basi insight of the dataframe will be obtained with help of pandas' library in-built functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Data aquisition\n",
    " * Dataset insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set comes in different formats like .csv, .json, .xlsx, sql etc. Dataset may be stored in our locoal computer or in web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I'll take the automobile dataset which is available in the web. It is a csv (Comma separated values) file and available at the following link:\n",
    " https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to import pandas into our working area\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aquition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First we have to import the dataset into jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import a dataset of the csv format from a web or local computer we use\n",
    "pandas.read_csv() \n",
    "function to read the csv file. In the bracket, we put the file path within quotation marks, so that pandas will read the file into a dataframe from that address. \n",
    "Pandas adds the top row as the headers, however, if there don't appear the column names on the top row of the dataset, but only data, then we can add an argument header = none inside read_csv() method and pandas will not assign the top row as headers columns but assign integers to each column in the imported dataframe starting with 0 continuing upto n-1 if there are n columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:Sometime header columns are not included in the dataset and listed separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset I am going to import does not include header columns, I'll add headers = None and pandas will not automatically set the first data row as a header instead assign numbers for each column. \n",
    "I can also assign the dataset a variable name appropriately on my own.\n",
    "To import xlsx, json types of file, we can use pd.read_excel, pd.read_json respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the online file by the URL provides above, and assign it to variable \"df\"\n",
    "URL_path = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/auto.csv\"\n",
    "df = pd.read_csv(URL_path, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After importing the dataset, we can use the method head(n) to look at the top n rows of the dataset, while tail(n) to look at the bottom n rows of the dataframe as follows:\n",
    "\n",
    "##### dataframe.head(n)\n",
    "##### dataframe.tail(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 5 rows using dataframe.head() method\n",
    "print(\"The first 5 rows of the dataframe\") \n",
    "df.head(5) # df.head() do the same purpose as df.head(5). However for n other than 5, we have to specify the n value within the bracket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the last 5 rows using dataframe.tail() method\n",
    "print(\"The first 5 rows of the dataframe\") \n",
    "df.tail(5) # df.tail() do the same purpose as tail(n). For n other than 5, we have to specify the n value within the bracket.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no header columns in the dataset and listed separately, we may want to assign the column names to the data frame manually. Let us see how we can assign column names manually to our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column information is available at https://archive.ics.uci.edu/ml/datasets/Automobile  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now add the headers manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll create a list \"headers\" that include all column names in order. Then, I'll use dataframe.columns = headers to replace the headers columns by the list we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create headers list\n",
    "headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
    "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
    "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
    "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n",
    "print(\"headers\\n\", headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can add the headers list to the dataframe as its headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = headers\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the column names in a dataset can be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below and press Shift+Enter to execute \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the modified dataframe in a new name in your local computer or in web.\n",
    "Define the file path or if it is in the same directory you are working, then simpley the file name with extension like .csv, .xlsx etc.\n",
    "\n",
    "Depending on the format we want to save, the file saving syntax will be also accordingly as follows pd.to_csv, pd.to_xlsx etc for csv and excel file respectively.  Here I am going to save the file in a name automobile.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"automobile.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading data into Pandas dataframe, we can explore the dataset.\n",
    "There are several ways to obtain essential insights of the data to help us better understand our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas dataframe, data is stored in different datatype forms:  Object, float, int, bool and datetime64 are main datatypes in which form data is stored. In order to better learn about each attribute of the dataset, it is always good to know the data type of each column. In Pandas we can know the datatypes of the different attributes by .dtypes methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the datatypes pandas determine are correct, however, for few it may be not, in such case we have to check and manully assign the datatype to that column. Here \"symbolling\", \"curb-weight\" are int64, \"normalized losses\" is object and \"wheel base\" is float64 etc, which needs to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical summary of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# describe() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a statistical summary of each column, we use the describe() method. This calculates the count, column mean value, column standard deviation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the statistical summary of all numeric (float and int) types columns. However, if want for object datatype, the above method does not give. For that We need to add an argument include = 'all' inside the bracket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the few columns of a dataframe and applying describe() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select columns of a dataframe by indicating the names of each column. For example:\n",
    "if we want to select the contents of column-1, column-2 and column-3 together, then  we can select as follows:\n",
    "dataframe[['column-1', 'column-2', 'column-3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can apply the describe() method in the new dataframe with the three columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset let us take the 'length' and 'curb-weight' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[['length', 'curb-weight']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['length', 'curb-weight']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info : a concise method to check the summary of a dataset\n",
    "It gives the top 30 rows and the bottom 30 rows of each column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is used as dataframe.info \n",
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it shows there are 205 rows and 26 columns in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete a column from the dataset\n",
    "\n",
    "We can drop a column/ from the dataframe by drop() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe.drop() is used to drop column/columns\n",
    "df.drop(['price'], axis=1) # we can drop multiple columns by listing  multple columns within the square brackets, for exa: df.drop(['numebr of doors','price'], axis = 1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
